# Customer Churn Analysis - End-to-End ML Project

A comprehensive machine learning project for predicting customer churn using advanced ML techniques, featuring a production-ready FastAPI web application.

## ğŸ¯ Project Overview

This project demonstrates a complete end-to-end machine learning pipeline for customer churn prediction, including:

- **Data Ingestion & Preprocessing**: Automated data cleaning, feature engineering, and handling missing values
- **Advanced ML Pipeline**: Multiple model training with hyperparameter tuning using GridSearchCV
- **Model Selection**: Automatic selection of best performing model with ensemble support
- **Dimension Reduction**: Multiple feature selection techniques (PCA, SelectKBest, Mutual Info, etc.)
- **Class Balancing**: SMOTE for handling imbalanced datasets
- **Model Evaluation**: Comprehensive metrics (Accuracy, Precision, Recall, F1, ROC-AUC)
- **Visualization**: Automated generation of performance visualizations
- **Production API**: FastAPI-based REST API with HTML interface for predictions
- **Docker Support**: Containerized deployment ready

## ğŸ“ Project Structure

```
Customer-Churn-Analysis/
â”œâ”€â”€ app/                    # FastAPI web application
â”‚   â”œâ”€â”€ api/               # API routes
â”‚   â”œâ”€â”€ core/              # Configuration and logging
â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â””â”€â”€ templates/         # HTML templates
â”œâ”€â”€ ml_pipeline/           # ML training pipeline
â”‚   â”œâ”€â”€ core/             # Shared utilities
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ dimension_reduction.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ model_evaluator.py
â”‚   â”œâ”€â”€ model_visualizer.py
â”‚   â””â”€â”€ advanced_pipeline.py
â”œâ”€â”€ data/                  # Data directory
â”‚   â””â”€â”€ raw/              # Raw data files
â”œâ”€â”€ models/                # Trained models and artifacts
â”‚   â””â”€â”€ visualizations/   # Generated plots
â”œâ”€â”€ tests/                 # Unit and integration tests
â”œâ”€â”€ logs/                  # Application logs
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ pyproject.toml         # Project configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile            # Docker configuration
â”œâ”€â”€ docker-compose.yml    # Docker Compose setup
â””â”€â”€ Makefile              # Common commands

```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- pip or uv
- (Optional) Docker and Docker Compose

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Customer-Churn-Analysis
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   # Or using uv (faster):
   uv pip install -r requirements.txt
   ```

4. **Prepare data**
   - Place your data file at `data/raw/data.csv`
   - Ensure the target column is named "Churn"

### Training the Model

```bash
# Using Makefile
make train

# Or directly
python train_model.py
```

The training pipeline will:
- Load and preprocess data
- Train multiple models (Random Forest, Gradient Boosting, SVM, etc.)
- Perform hyperparameter tuning with GridSearchCV
- Select the best model
- Create an ensemble (optional)
- Generate evaluation reports and visualizations
- Save the best model with dynamic naming

### Running the API

```bash
# Using Makefile
make run

# Or directly
python run.py
# Or
uvicorn app.main:app --reload
```

Access the API at:
- **API Documentation**: http://localhost:8000/docs
- **Web Interface**: http://localhost:8000
- **ReDoc**: http://localhost:8000/redoc

### Using Docker

```bash
# Build and run with Docker Compose
make docker-compose-up

# Or build and run manually
make docker-build
make docker-run
```

## ğŸ“Š Model Training Details

### Supported Models

- Random Forest
- Gradient Boosting
- AdaBoost
- Extra Trees
- Logistic Regression
- SVM
- K-Nearest Neighbors
- Naive Bayes
- Decision Tree
- Neural Network (MLP)

### Feature Engineering

- One-hot encoding for categorical variables
- Missing value imputation (median for numerical, mode for categorical)
- Feature scaling (StandardScaler)
- Dimension reduction (SelectFromModel, PCA, SelectKBest, etc.)

### Model Selection

The pipeline automatically:
1. Trains all models with optimized hyperparameters
2. Evaluates each model using cross-validation
3. Selects the best performing model
4. Optionally creates an ensemble of top 3 models
5. Saves the best model with dynamic naming (e.g., `random_forest_model.joblib`)

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run with coverage
pytest tests/ -v --cov

# Run specific test
pytest tests/test_data_ingestion.py -v
```

## ğŸ”§ Development

### Code Quality

```bash
# Format code
make format

# Lint code
make lint
```

### Project Commands

```bash
make help          # Show all available commands
make install       # Install dependencies
make train         # Train the model
make test          # Run tests
make lint          # Run linter
make format        # Format code
make clean         # Clean temporary files
```

## ğŸ“ˆ Model Performance

After training, check:
- `models/evaluation_report.json` - Detailed metrics
- `models/model_comparison.json` - Model comparison
- `models/visualizations/` - Performance plots

## ğŸ³ Docker Deployment

The project includes:
- Multi-stage Dockerfile for optimized builds
- Docker Compose for easy deployment
- Production-ready configuration

## ğŸ“š Documentation

- [CI/CD Guide](docs/CI_CD.md) - Continuous Integration/Deployment
- [Docker Guide](docs/DOCKER.md) - Docker setup and deployment
- [Dimension Reduction](docs/DIMENSION_REDUCTION.md) - Feature selection techniques
- [Visualizations](docs/VISUALIZATIONS.md) - Understanding model outputs

## ğŸ—ï¸ Architecture

### ML Pipeline Flow

```
Data Ingestion â†’ Preprocessing â†’ Feature Engineering â†’ 
Dimension Reduction â†’ Model Training â†’ Evaluation â†’ 
Model Selection â†’ Visualization â†’ Model Persistence
```

### API Architecture

```
FastAPI Application
â”œâ”€â”€ Routes (API endpoints)
â”œâ”€â”€ Services (Business logic)
â”‚   â”œâ”€â”€ Prediction Service
â”‚   â””â”€â”€ Data Processor
â””â”€â”€ Core (Configuration, Logging)
```

## ğŸ” Configuration

Configuration is managed through:
- Environment variables (`.env` file)
- `app/core/config.py` - Application settings
- `pyproject.toml` - Project metadata

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)

## ğŸ™ Acknowledgments

- scikit-learn for ML algorithms
- FastAPI for the web framework
- All open-source contributors

## ğŸ“Š Project Highlights

âœ… **End-to-End Pipeline**: Complete ML workflow from data to deployment  
âœ… **Production Ready**: FastAPI API with Docker support  
âœ… **Best Practices**: Clean code, testing, documentation  
âœ… **Scalable**: Modular architecture, easy to extend  
âœ… **Comprehensive**: Multiple models, hyperparameter tuning, ensemble methods  
âœ… **Visualization**: Automated performance plots  
âœ… **Documentation**: Well-documented code and guides  

---

**Note**: This project demonstrates professional ML engineering practices suitable for production environments.
